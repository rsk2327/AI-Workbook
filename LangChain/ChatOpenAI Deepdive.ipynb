{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e4b519-0d55-48b2-99e9-62a9d87f4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28cf862-4289-48a8-be5a-467314ed7ab6",
   "metadata": {},
   "source": [
    "# ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7216c7e-0fde-4633-a024-7a0369705a04",
   "metadata": {},
   "source": [
    "## Model Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd83e6c-9561-4943-8e1e-f29c479b508a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am a language model AI created by OpenAI and I do not have a personal name. You can call me Assistant or AI. How can I assist you today?', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(temperature=0.0)\n",
    "input_string = \"What is your name?\"\n",
    "\n",
    "\n",
    "model.invoke(input_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ba834-2465-42c8-9c2f-fab98b172872",
   "metadata": {},
   "source": [
    "## Function Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ffec9-aea8-425e-aa6f-a06f53d25b70",
   "metadata": {},
   "source": [
    "### Step 1 : Convert input string into PromptValue object\n",
    "In this case, its converted specifically to subclass StringPromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf952ca-1799-4330-954e-999e6411bf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='What is your name?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt_value = model._convert_input(input_string)\n",
    "input_prompt_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1fd0c-786e-4afb-8722-9a02d6c8d2cf",
   "metadata": {},
   "source": [
    "### Step 2 : Convert PromptValue object into corresponding BaseMessage object\n",
    "Again, in this case its converted specifically to subclass HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2929f34b-4c83-4a9c-8613-ebfc1715a1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[HumanMessage(content='What is your name?')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt_messages = [p.to_messages() for p in [input_prompt_value]]   # Adding the list syntax as generate_prompt only takes a list as input\n",
    "input_prompt_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f44cfd2-3224-46dd-a184-8358ef6f3cb8",
   "metadata": {},
   "source": [
    "### Step 3 : Convert the BaseMessage object into a dict format\n",
    "The dict format is dependent on the input format of OpenAI API\n",
    "For other APIs, this step would have to be adapted to account for the specific API input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566cf437-64ae-4b84-94bb-7d2a6d97eab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'What is your name?'}]\n",
      "{'model': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0}\n"
     ]
    }
   ],
   "source": [
    "input_message_openai_format, params = model._create_message_dicts(input_prompt_messages[0], stop = None )\n",
    "print(input_message_openai_format)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad4e43-0de5-425d-9b7e-bd9e6c6c4d7c",
   "metadata": {},
   "source": [
    "### Step 4 : Push the input dict value to the OpenAI client\n",
    "The OpenAI API is accessed through the model.client variable. Specically, model.client is an instance of openai.chat.completions.Completions in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df6c8ac7-2c8b-4fa3-b8a8-b41c7c69af01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-95LY56GyGV2sqOuACqM94p9lhOBIf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I am a language model AI created by OpenAI, so I do not have a personal name. You can call me Assistant or AI. How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1711061501, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f0b692a78', usage=CompletionUsage(completion_tokens=35, prompt_tokens=12, total_tokens=47))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_result = model.client.create(messages=input_message_openai_format, **params)\n",
    "openai_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70d7b6-8651-4d6a-9bc5-9095ae2fc103",
   "metadata": {},
   "source": [
    "### Step 5 : Convert the output from OpenAI client into a ChatResult object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19062b67-27de-453e-9543-6e831845ee1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResult(generations=[ChatGeneration(text='I am a language model AI created by OpenAI, so I do not have a personal name. You can call me Assistant or AI. How can I assist you today?', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='I am a language model AI created by OpenAI, so I do not have a personal name. You can call me Assistant or AI. How can I assist you today?'))], llm_output={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 12, 'total_tokens': 47}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_output = model._create_chat_result(openai_result)\n",
    "chat_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0a461-692d-4634-bd71-7a42b58cd7e8",
   "metadata": {},
   "source": [
    "### Step 6 : Extract the final message from the ChatResult object\n",
    "The output will be an instance of the subclass AIMessage as it represents the output from the LLM/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec6b5b6-0372-48d8-93b7-d087c3a25bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am a language model AI created by OpenAI, so I do not have a personal name. You can call me Assistant or AI. How can I assist you today?')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output = chat_output.generations[0].message\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7358ba-3146-431a-97a5-0a66002f67f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f66576-5453-4813-8ab5-093eec85a6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6513b841-10e1-425a-8dc1-41bf5d32ac56",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4b4cb2-dbb3-4b28-8508-a6616883dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e18501-e08c-4616-9baf-d368ebbbb290",
   "metadata": {},
   "source": [
    "### Implementation with ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02be993e-3253-4ed6-b48d-f815a69f0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "chat_chain = ConversationChain(\n",
    "    llm=model, \n",
    "    memory = ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b0c518-e97c-4e66-a582-0960439b2145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi. My name is Roshan. I need some help',\n",
       " 'history': '',\n",
       " 'response': \"Hello Roshan! I'm here to help. What do you need assistance with?\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain(\"Hi. My name is Roshan. I need some help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807fb6ac-84c6-4d92-833e-38d8d90f2579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Can you tell me what my name is?',\n",
       " 'history': \"Human: Hi. My name is Roshan. I need some help\\nAI: Hello Roshan! I'm here to help. What do you need assistance with?\",\n",
       " 'response': 'Of course, your name is Roshan. It\\'s a beautiful name that means \"light\" or \"bright\" in Sanskrit. It\\'s great to meet you, Roshan! How can I assist you further?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain(\"Can you tell me what my name is?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78847be4-89da-47ae-8429-37d8cb28b631",
   "metadata": {},
   "source": [
    "### Implementation without ConversationChain\n",
    "\n",
    "ConversationChain is a sub-class of LLMChain. Barring a fixed prompt template and providing some checks on the input format, ConversationChain doesnt add any additional value on top of LLMChain. So we can implement the same workflow as above replacing ConversationChain with LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b6be6d4-45c2-4c69-89ee-4fed6f0c0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chat_chain = LLMChain(\n",
    "    llm=model, \n",
    "    memory = ConversationBufferMemory(),\n",
    "    prompt = prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58fb8bba-653e-4a23-86a6-3e53c2d2c2bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi. My name is Shweta',\n",
       " 'history': '',\n",
       " 'text': \"Hello Shweta! It's nice to meet you. How can I assist you today?\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke(\"Hi. My name is Shweta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78efc7a4-aa5b-4646-8590-e6309d4164a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Can you tell me what my name is?',\n",
       " 'history': \"Human: Hi. My name is Shweta\\nAI: Hello Shweta! It's nice to meet you. How can I assist you today?\",\n",
       " 'text': 'Of course, Shweta! Your name is Shweta. It\\'s a beautiful name of Indian origin that means \"white\" or \"pure.\" Is there anything else you would like to know or talk about?'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke(\"Can you tell me what my name is?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a78964-45bc-4253-8a4b-e29be6cb845a",
   "metadata": {},
   "source": [
    "## Fixed Prompt\n",
    "\n",
    "One of the specifications of the ConversationChain class is the use of a fixed prompt template. This prompt is accessed through langchain.chains.conversation.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17324892-6169-411b-b670-ac64fcb3c93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.conversation.prompt import PROMPT \n",
    "PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa734bae-a51d-462d-b105-a29a26c53ee8",
   "metadata": {},
   "source": [
    "### Adding a different Persona to the ConversationChain bot\n",
    "\n",
    "If we want to add some variation to the ConversationChain bot, we can do so by modifying the base prompt template thats used here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "defca835-703e-4ee9-98e7-d6b2c740d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a rapper having a conversation with a friend. You tend to respond to questions in the form of a rap.\n",
    "\n",
    "\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chat_chain = ConversationChain(\n",
    "    llm=model, \n",
    "    memory = ConversationBufferMemory(),\n",
    "    prompt = prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f1c4956-1abf-4853-86b1-a7fa8ecd21ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi. My name is Roshan',\n",
       " 'history': '',\n",
       " 'response': \"Yo, what's up Roshan, how you doin' today?\\nTell me what's on your mind, what you gotta say?\\nI'm here to listen, I'm here to vibe\\nLet's chat it up, let's take this ride\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_chain.invoke(\"Hi. My name is Roshan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b79709-1ea9-444d-aeea-a511b4f72fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bdc4af-410d-4c16-b4b3-5cdaabd3139d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46bac8-fce4-4094-897f-cbb4ac776a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7a3d7-5b8b-4227-843e-0bd9c83917c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c8dfa-b58a-4ece-8d28-f827f0601bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5477209-029e-4c49-a143-90b19b28c045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2666685-dc05-41e0-97c8-4f7037d2b17b",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "847498a5-caab-4642-bdbd-6aae156c5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompt_values import ChatPromptValue, PromptValue, StringPromptValue\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b619481-084c-467e-b2ef-caca43cbc8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Answer the following question: \\n{question}\\n'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question: \n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8f231-da5d-4a6c-a419-47f8fae4ee4c",
   "metadata": {},
   "source": [
    "input_variables : Lists all the placeholders that will be filled in later with user input\n",
    "\n",
    "messages : Lists the single message prompt template that will be outputted from this prompt template. The key thing to note here is that the message is of the class HumanMessagePromptTemplate. This is a default functionality of the template. If we only provide a single string with input variables to be filled in, its assumed that this is the prompt template for the Human/User thats part of the chat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a7757-cef4-464b-9e09-e3b0955477e9",
   "metadata": {},
   "source": [
    "On passing an input value to the prompt template, we now have a prompt value (ChatPromptValue object). Looking specifically at the messages value, it now has a HumanMessage object instead of the HumanMessagePromptTemplate object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a429e125-d271-4a35-adcc-82225d073c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Answer the following question: \\nWhat is your name?\\n')])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt_template.invoke({\"question\":\"What is your name?\"}) \n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3895a3-525d-466f-85e9-0dfd70229a25",
   "metadata": {},
   "source": [
    "Alternative options to extract final prompt from prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f5d1385-1cb6-4815-823a-6bd73482cd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Answer the following question: \\nWhat is your name?\\n')])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#format_prompt returns ChatPromptValue object\n",
    "output = prompt_template.format_prompt(question = \"What is your name?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e63dd21a-b7b4-4caa-94e8-9d7c813d193d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Answer the following question: \\nWhat is your name?\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternatively, the format method returns string object\n",
    "output = prompt_template.format(question = \"What is your name?\")\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
