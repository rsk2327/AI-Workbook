{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400de936-34c8-47df-98ef-3c4aaf63bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from io import StringIO \n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import sys  \n",
    "sys.path.append( '/Users/roshansk/Documents/GitHub/langchain/libs/langchain')\n",
    "sys.path.append( '/Users/roshansk/Documents/GitHub/langchain/libs/experimental/')\n",
    "sys.path.append( '/Users/roshansk/Documents/GitHub/langchain/libs/partners/openai')\n",
    "sys.path.append( '/Users/roshansk/Documents/GitHub/langchain/libs/community/langchain_community/')\n",
    "\n",
    "\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS, Chroma, Milvus\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import langchain_community\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterable,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sized,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d73786-61dd-4126-871b-388f35789a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af250de6-179b-43c1-a2c3-0c091ac77840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90502e-a753-4dc1-9204-1d854f42ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a881d-06a6-4542-9d56-879f0f23200f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4acdf-15fa-46f6-80d5-55fdefb222fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForRetrieverRun,\n",
    "    CallbackManagerForRetrieverRun,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import Field, root_validator\n",
    "from typing import Dict, List, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.langchain_community.vectorstores.faiss import dependable_faiss_import\n",
    "\n",
    "\n",
    "class SentenceWindowRetriever(BaseRetriever):\n",
    "    \"\"\"`Sentence Window` retriever.\n",
    "\n",
    "    Sentence Window Retriever dissociates retrieval from generation\n",
    "    by appending adjacent chunks to the retrieved chunk to provide\n",
    "    additional context to the generation model\n",
    "\n",
    "    Currently, it supports 2 backends:\n",
    "    FAISS, Chroma\n",
    "    \"\"\"\n",
    "\n",
    "    store: VectorStore\n",
    "    \"\"\"VectorStore containing the chunks of text. Only supports the \n",
    "        above listed databases\"\"\"\n",
    "\n",
    "    k: int = 4\n",
    "    \"\"\"Number of top results to return\"\"\"\n",
    "\n",
    "    window_size: int = 2\n",
    "    \"\"\"Number of adjacent chunks on each side to be added \"\"\"\n",
    "\n",
    "    @root_validator\n",
    "    def check_database_type(cls, values: Dict) -> Dict:\n",
    "        k = values.get(\"k\", -1)\n",
    "        store = values.get(\"store\")\n",
    "\n",
    "        if k < 0:\n",
    "            raise ValueError(\"The value of k must be greater than 0\")\n",
    "\n",
    "        if store in \n",
    "\n",
    "        return values\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "\n",
    "        if type(self.store) == Chroma:\n",
    "            return self._sentence_window_retriever_chroma(query=query)\n",
    "        # elif type(self.store) == PineconeVectorStore:\n",
    "        #     return self._sentence_window_retriever_pinecone(query=query)\n",
    "        elif type(self.store) == FAISS:\n",
    "            return self._sentence_window_retriever_faiss(query=query)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"\"\"Only the following databases are currently supported for \n",
    "                the implementation of Sentence Window Retriever : \n",
    "                FAISS, Chroma\"\"\"\n",
    "            )\n",
    "\n",
    "    def _sentence_window_retriever(self, query: str) -> List[Document]:\n",
    "\n",
    "        vector = self.store.embeddings.embed_query(query)  # type: ignore\n",
    "\n",
    "        \n",
    "\n",
    "    def _sentence_window_retriever_chroma(self, query: str) -> List[Document]:\n",
    "        assert isinstance(self.store, Chroma)\n",
    "\n",
    "        vector = self.store.embeddings.embed_query(query)  # type: ignore\n",
    "\n",
    "        top_results = self.store._collection.query(\n",
    "            query_embeddings=vector, n_results=self.k\n",
    "        )\n",
    "\n",
    "        doc_list = []\n",
    "\n",
    "        for id, metadata in zip(top_results[\"ids\"][0], top_results[\"metadatas\"][0]):  # type: ignore\n",
    "            primary_index = metadata.get(\"chunk_id\")\n",
    "            source_text = metadata.get(\"source\")\n",
    "            start_index = max(0, primary_index - self.window_size)\n",
    "            end_index = primary_index + self.window_size\n",
    "\n",
    "            if source_text:\n",
    "                output = self.store._collection.get(\n",
    "                    where={\n",
    "                        \"$and\": [\n",
    "                            {\"source\": source_text},\n",
    "                            {\"chunk_id\": {\"$gte\": start_index}},\n",
    "                            {\"chunk_id\": {\"$lte\": end_index}},\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                output = self.store._collection.get(\n",
    "                    where={\n",
    "                        \"$and\": [\n",
    "                            {\"chunk_id\": {\"$gte\": start_index}},\n",
    "                            {\"chunk_id\": {\"$lte\": end_index}},\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            page_content = \" \".join(output[\"documents\"])  # type: ignore\n",
    "            pages = [x.get(\"page\") for x in output[\"metadatas\"]]  # type: ignore\n",
    "\n",
    "            metadata = {\n",
    "                \"primary_index\": primary_index,\n",
    "                \"chroma_id\": id,\n",
    "                \"page\": list(set(pages)),\n",
    "                \"type\": \"sentence_window\",\n",
    "            }\n",
    "\n",
    "            if source_text:\n",
    "                metadata[\"source\"] = source_text\n",
    "\n",
    "            output_doc = Document(page_content=page_content, metadata=metadata)\n",
    "\n",
    "            doc_list.append(output_doc)\n",
    "\n",
    "        return doc_list\n",
    "\n",
    "    def _sentence_window_retriever_pinecone(self, query: str) -> List[Document]:\n",
    "        assert isinstance(self.store, PineconeVectorStore)\n",
    "\n",
    "        top_results = self.store.similarity_search(query, k=self.k)\n",
    "\n",
    "        vector_dimension = self.store._index.describe_index_stats()[\"dimension\"]\n",
    "\n",
    "        doc_list = []\n",
    "\n",
    "        for doc in top_results:\n",
    "            primary_index = doc.metadata.get(\"chunk_id\")\n",
    "\n",
    "            if primary_index is None:\n",
    "                raise ValueError(\n",
    "                    \"chunk_id metadata variable is missing in retrieved documents\"\n",
    "                )\n",
    "\n",
    "            source_text = doc.metadata.get(\"source\")\n",
    "\n",
    "            start_index = max(0, primary_index - self.window_size)\n",
    "            end_index = primary_index + self.window_size\n",
    "\n",
    "            if source_text:\n",
    "                filter_value = {\n",
    "                    \"$and\": [\n",
    "                        {\"source\": source_text},\n",
    "                        {\"chunk_id\": {\"$gte\": start_index}},\n",
    "                        {\"chunk_id\": {\"$lte\": end_index}},\n",
    "                    ]\n",
    "                }\n",
    "            else:\n",
    "                filter_value = {\n",
    "                    \"$and\": [\n",
    "                        {\"chunk_id\": {\"$gte\": start_index}},\n",
    "                        {\"chunk_id\": {\"$lte\": end_index}},\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "            output = self.store._index.query(\n",
    "                vector=[0] * vector_dimension,\n",
    "                top_k=2 * self.window_size + 1,\n",
    "                filter=filter_value,\n",
    "                include_metadata=True,\n",
    "            )\n",
    "\n",
    "            output = sorted(output[\"matches\"], key=lambda x: x[\"metadata\"][\"chunk_id\"])\n",
    "\n",
    "            page_content = \" \".join([x[\"metadata\"].get(\"text\") for x in output])\n",
    "            pages = [x[\"metadata\"].get(\"page\") for x in output]\n",
    "\n",
    "            metadata = {\n",
    "                \"primary_index\": primary_index,\n",
    "                \"page\": list(set(pages)),\n",
    "                \"type\": \"sentence_window\",\n",
    "            }\n",
    "\n",
    "            if source_text:\n",
    "                metadata[\"source\"] = source_text\n",
    "\n",
    "            output_doc = Document(page_content=page_content, metadata=metadata)\n",
    "\n",
    "            doc_list.append(output_doc)\n",
    "\n",
    "        return doc_list\n",
    "\n",
    "    def _sentence_window_retriever_faiss(self, query: str) -> List[Document]:\n",
    "        faiss = dependable_faiss_import()\n",
    "\n",
    "        assert isinstance(self.store, FAISS)\n",
    "\n",
    "        vector_ = self.store._embed_query(query)\n",
    "        vector = np.array([vector_], dtype=np.float32)\n",
    "        if self.store._normalize_L2:\n",
    "            faiss.normalize_L2(vector)\n",
    "\n",
    "        # Retrieve the top k indices that matches the query\n",
    "        scores, indices = self.store.index.search(vector, self.k)\n",
    "\n",
    "        doc_list = []\n",
    "        for primary_index in indices[0]:\n",
    "            start_index = max(0, primary_index - self.window_size)\n",
    "            end_index = min(\n",
    "                primary_index + self.window_size,\n",
    "                len(self.store.index_to_docstore_id) - 1,\n",
    "            )\n",
    "            primary_doc = self.store.docstore.search(\n",
    "                self.store.index_to_docstore_id[primary_index]\n",
    "            )\n",
    "\n",
    "            assert isinstance(primary_doc, Document)\n",
    "            primary_doc_source = primary_doc.metadata.get(\"source\", None)\n",
    "\n",
    "            # Retrieving content for neighboring indices\n",
    "            page_content_list = []\n",
    "            pages = []\n",
    "            for index in range(start_index, end_index + 1):\n",
    "                doc = self.store.docstore.search(self.store.index_to_docstore_id[index])\n",
    "                assert isinstance(doc, Document)\n",
    "\n",
    "                if doc.metadata.get(\"source\", None) == primary_doc_source:\n",
    "                    # We only want to include adjacent indices that are\n",
    "                    # from the same source text. If source is not provided,\n",
    "                    # then this condition is relaxed\n",
    "                    page_content_list.append(doc.page_content)\n",
    "                    pages.append(doc.metadata[\"page\"])\n",
    "\n",
    "            # Creating new output Document\n",
    "            assert isinstance(doc, Document)\n",
    "\n",
    "            page_content = \" \".join(page_content_list)\n",
    "\n",
    "            metadata = {\n",
    "                \"primary_index\": primary_index,\n",
    "                \"page\": list(set(pages)),\n",
    "                \"type\": \"sentence_window\",\n",
    "            }\n",
    "\n",
    "            if primary_doc_source:\n",
    "                metadata[\"source\"] = primary_doc_source\n",
    "\n",
    "            output_doc = Document(page_content=page_content, metadata=metadata)\n",
    "\n",
    "            doc_list.append(output_doc)\n",
    "\n",
    "        return doc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11da15-f082-414e-a29a-4c3a50d7f3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8089f66-855b-4d24-87a8-562598994c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66289f82-0557-4d12-b99e-d49645c3c5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8a4e6-d865-4c53-a927-e76a973d9707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a89b89f-1468-4eba-b109-82d4bb98a724",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ce2aed-c338-422a-b240-c43b97040c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Users/roshansk/Documents/AI/AdobeTest/test_data/pdfs/DR--110685614.pdf'\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(file)\n",
    "doc = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index = True, add_chunk_id = True)\n",
    "\n",
    "splits = splitter.split_documents(doc)\n",
    "\n",
    "query = \"Hey there\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbff253-66a7-46d9-a452-eafdc8f66414",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c26360-bd90-45c4-b6b0-2bff22d93ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idList = [str(x) for x in list(range(len(splits)))]\n",
    "\n",
    "store = Chroma.from_documents(splits, OpenAIEmbeddings(), ids = idList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3f243e-0628-4545-8af3-716afc617134",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = store.embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1dc4eda-e099-46b0-87e1-aedab2ec9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = store.similarity_search_by_vector(embedding=vector, k = 4, include_id = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5444c6e-ca8e-4448-8900-3f87a23ee165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='7  7  7  7  7  \\n          \\n8  8  8  8  8  \\n          \\n9  9  9  9  9  \\n          \\n10  10  10  10  10  \\n          \\n11  11  11  11  11  \\n          \\n12  12  12  12  12  \\n          \\n1  1  1  1  1  \\n          \\n2  2  2  2  2  \\n          \\n3  3  3  3  3  \\n          \\n4  4  4  4  4  \\n          \\n5  5  5  5  5  \\n          \\n6  6  6  6  6  \\n          \\n Evening   Evening   Evening   Evening   Evening', metadata={'chunk_id': 15, 'page': 3, 'source': '/Users/roshansk/Documents/AI/AdobeTest/test_data/pdfs/DR--110685614.pdf', 'start_index': 226, 'id': '15'}),\n",
       " Document(page_content='6  6  6  6  6  \\n          \\n Evening   Evening   Evening   Evening   Evening  \\n \\nTask 3:  You are meeting friends  downtown for dinner at 6 p .m. on Wednesday. What time does the bus \\nleave the college after your last class on Wednesday?  \\nThe bus leaves at 5:20 pm  \\n \\nTask 4:   Name the two locations where your classes will be held according to the class schedule.', metadata={'chunk_id': 33, 'page': 6, 'source': '/Users/roshansk/Documents/AI/AdobeTest/test_data/pdfs/DR--110685614.pdf', 'start_index': 1175, 'id': '33'}),\n",
       " Document(page_content='before it begins.  \\n \\nTask 3:  You are meeting friends  downtown  for dinner at 6 p.m. on Wednesday. What time does \\nthe bus leave after your last class on Wednesday  that would allow you to meet your \\nfriends on time ?  \\n \\n \\nTask 4:  Name the two building s where  your classes will be held according to the class schedule.', metadata={'chunk_id': 12, 'page': 1, 'source': '/Users/roshansk/Documents/AI/AdobeTest/test_data/pdfs/DR--110685614.pdf', 'start_index': 980, 'id': '12'}),\n",
       " Document(page_content='leave the college after your last class on Wednesday?  \\nThe bus leaves at 5:20 pm  \\n \\nTask 4:   Name the two locations where your classes will be held according to the class schedule.  \\nBrealey and PTBO Sport and Wellness Centre', metadata={'chunk_id': 34, 'page': 6, 'source': '/Users/roshansk/Documents/AI/AdobeTest/test_data/pdfs/DR--110685614.pdf', 'start_index': 1358, 'id': '34'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b92e8b4-a305-47d7-a603-d272345ae35c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da314d1-5dab-40f2-a56e-0aa10ff95c65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='7  7  7  7  7  \\n          \\n8  8  8  8  8  \\n          \\n9  9  9  9  9  \\n          \\n10  10  10  10  10  \\n          \\n11  11  11  11  11  \\n          \\n12  12  12  12  12  \\n          \\n1  1  1  1  1  \\n          \\n2  2  2  2  2  \\n          \\n3  3  3  3  3  \\n          \\n4  4  4  4  4  \\n          \\n5  5  5  5  5  \\n          \\n6  6  6  6  6  \\n          \\n Evening   Evening   Evening   Evening   Evening', metadata={'chunk_id': 15, 'page': 3, 'source': '/Users/roshansk/Documents/AI/AdobeTest/test_data/pdfs/DR--110685614.pdf', 'start_index': 226, 'id': '15'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea55e795-a296-461e-8769-9d4357ff6ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_id = int(result.metadata.get('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5716fc6-e06c-43ac-a227-4b97550d0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "start_index = result_id - window_size\n",
    "end_index = result_id + window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f79bcfb-7f8b-4dfb-a603-207df6c92899",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_vectors = store.get_documents_by_ids(list(range(start_index, end_index+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60e38d1c-c550-43d1-adb2-6d76c688f7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d082a266-6a4a-46ab-9a4d-addacc473e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_window_vectors(vector, window_vectors):\n",
    "\n",
    "    source_text = vector.metadata.get('source')\n",
    "\n",
    "    page_content = []\n",
    "    pages = []\n",
    "    for doc in window_vectors:\n",
    "    \n",
    "        if source_text == doc.metadata.get('source'):\n",
    "            page_content.append(doc.page_content)\n",
    "            pages.append(doc.metadata.get('page'))\n",
    "    \n",
    "    metadata = {'source': source_text, \n",
    "                   'pages':list(set(pages)),\n",
    "                    \"type\": \"sentence_window\",\n",
    "                    \"primary_id\": vector.metadata.get('id')\n",
    "                   }\n",
    "    page_content = \" \".join(page_content)\n",
    "    \n",
    "    return Document(page_content=page_content, metadata = metadata)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89dc8565-392c-459d-991c-29bdfbcdd05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Transition Task: Prepared for the Project,  Teaching to Fish (Build Tasks) Integrating OALCF Task \\nDevelopment within Ontario’s Literacy Programs  (2014)  \\n \\n3 \\n Class Schedule Transition Task: Prepared for the Project,  Teaching to Fish (Build Tasks) Integrating OALCF Task \\nDevelopment within Ontario’s Literacy Programs  (2014)  \\n \\n4 \\n Weekly agenda  \\nMonday  Tuesday   Wednesday  Thursday  Friday  \\n7  7  7  7  7  \\n          \\n8  8  8  8  8  \\n          \\n9  9  9  9  9  \\n          \\n10  10  10  10  10  \\n          \\n11  11  11  11  11  \\n          \\n12  12  12  12  12 7  7  7  7  7  \\n          \\n8  8  8  8  8  \\n          \\n9  9  9  9  9  \\n          \\n10  10  10  10  10  \\n          \\n11  11  11  11  11  \\n          \\n12  12  12  12  12  \\n          \\n1  1  1  1  1  \\n          \\n2  2  2  2  2  \\n          \\n3  3  3  3  3  \\n          \\n4  4  4  4  4  \\n          \\n5  5  5  5  5  \\n          \\n6  6  6  6  6  \\n          \\n Evening   Evening   Evening   Evening   Evening Transition Task: Prepared for the Project,  Teaching to Fish (Build Tasks) Integrating OALCF Task \\nDevelopment within Ontario’s Literacy Programs  (2014)  \\n \\n5 \\n The Express Bus Schedule  \\nDepart Downtown  Arrive at College  Depart College  Arrive Downtown  \\n6:30 am  6:55 am  7:00 am  7:25 am  \\n7:30 am  7:55 am  8:00 am  8:25 am \\n8:30 am  8:55 am  9:00 am  9:25 am Depart Downtown  Arrive at College  Depart College  Arrive Downtown  \\n6:30 am  6:55 am  7:00 am  7:25 am  \\n7:30 am  7:55 am  8:00 am  8:25 am \\n8:30 am  8:55 am  9:00 am  9:25 am  \\n9:30 am  9:55 am  10:00 am  10:25 am  \\n10:30 am  10:55 am  11:00 am  11:25 am  \\n1:30 pm  1:55 pm  2:00 pm  2:25 pm  \\n2:30 pm  2:55 pm  3:00 pm  3:25 pm  \\n3:30 pm  3:55 pm  4:00 pm  4:25 pm', metadata={'source': '/Users/roshansk/Documents/AI/AdobeTest/test_data/pdfs/DR--110685614.pdf', 'pages': [2, 3, 4], 'type': 'sentence_window', 'primary_id': '15'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_window_vectors(result, window_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821933aa-4465-4e3e-b2dc-092dae81db20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_dev",
   "language": "python",
   "name": "langchain_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
